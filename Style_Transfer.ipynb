{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_Transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4h_sBU9PL0W6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications import vgg19\n",
        "from keras.layers import Conv2D\n",
        "from keras import backend as K\n",
        "from keras.models import load_model, Sequential, Model\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n",
        "# parser.add_argument('base_image_path', metavar='base', type=str,\n",
        "#                     help='Path to the image to transform.')\n",
        "# parser.add_argument('style_reference_image_path', metavar='ref', type=str,\n",
        "#                     help='Path to the style reference image.')\n",
        "# parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n",
        "#                     help='Prefix for the saved results.')\n",
        "# parser.add_argument('--iter', type=int, default=10, required=False,\n",
        "#                     help='Number of iterations to run.')\n",
        "# parser.add_argument('--content_weight', type=float, default=0.025, required=False,\n",
        "#                     help='Content weight.')\n",
        "# parser.add_argument('--style_weight', type=float, default=1.0, required=False,\n",
        "#                     help='Style weight.')\n",
        "# parser.add_argument('--tv_weight', type=float, default=1.0, required=False,\n",
        "#                     help='Total Variation weight.')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# base_image_path = args.base_image_path\n",
        "# style_reference_image_path = args.style_reference_image_path\n",
        "# result_prefix = args.result_prefix\n",
        "# iterations = args.iter\n",
        "\n",
        "# # these are the weights of the different loss components\n",
        "# total_variation_weight = args.tv_weight\n",
        "# style_weight = args.style_weight\n",
        "# content_weight = args.content_weight\n",
        "\n",
        "base_image_path = 'city.jpg'\n",
        "style_reference_image_path = 'starry-night.jpg'\n",
        "result_prefix = 'output'\n",
        "iterations = 10\n",
        "\n",
        "total_variation_weight = 1.0\n",
        "style_weight = 1.0\n",
        "content_weight = 0.025\n",
        "\n",
        "\n",
        "# dimensions of the generated picture.\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)\n",
        "\n",
        "# util function to open, resize and format pictures into appropriate tensors\n",
        "\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((3, img_nrows, img_ncols))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "# get tensor representations of our images\n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
        "\n",
        "# this will contain our generated image\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
        "else:\n",
        "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
        "\n",
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)\n",
        "\n",
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "vgg = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "x = vgg.output\n",
        "x = Conv2D(strides = 2, kernel_size = 5, filters = 512, use_bias = True, bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05), \n",
        "           padding = \"valid\", activation = tf.nn.relu)(x)\n",
        "\n",
        "model = Model(inputs = vgg.inputs, outputs = x)\n",
        "print(model.summary())\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "print('Model loaded.')\n",
        "\n",
        "\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "# compute the neural style loss\n",
        "# first we need to define 4 util functions\n",
        "\n",
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "    assert K.ndim(x) == 3\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        features = K.batch_flatten(x)\n",
        "    else:\n",
        "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram\n",
        "\n",
        "# the \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "\n",
        "\n",
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))\n",
        "\n",
        "# the 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        a = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
        "        b = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
        "    else:\n",
        "        a = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "        b = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))\n",
        "\n",
        "# combine these loss functions into a single scalar\n",
        "loss = K.variable(0.0)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#layer_features = outputs_dict['block5_conv2']\n",
        "layer_features = outputs_dict['conv2d_3']\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss += content_weight * content_loss(base_image_features,\n",
        "                                      combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss += (style_weight / len(feature_layers)) * sl\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "# get the gradients of the generated image wrt the loss\n",
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)\n",
        "\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
        "    else:\n",
        "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient.\n",
        "\n",
        "\n",
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "img = x\n",
        "fname = ''\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JY9lblnL21s",
        "colab_type": "code",
        "outputId": "60c53677-09fb-4e82-cd74-9d4fa316d220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1798
        }
      },
      "cell_type": "code",
      "source": [
        "Travgg = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "\n",
        "from keras.models import load_model, Sequential, Model\n",
        "from keras.layers import Dense, Conv2D\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# prev_model = vgg # loading the previously saved model.\n",
        "\n",
        "# top_model = Sequential()\n",
        "# top_model.add(Dense(512,activation='relu'))\n",
        "\n",
        "# model = Model(inputs=prev_model.input, outputs=top_model(prev_model.output))\n",
        "x = vgg.output\n",
        "x = Conv2D(strides = 2, kernel_size = 5, filters = 512, use_bias = True, bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05), \n",
        "           padding = \"valid\", activation = tf.nn.relu)(x)\n",
        "\n",
        "final_model = Model(inputs = vgg.inputs, outputs = x)\n",
        "print(final_model.summary())\n",
        "\n",
        "print(vgg.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, None, None, 12)    153612    \n",
            "=================================================================\n",
            "Total params: 20,177,996\n",
            "Trainable params: 20,177,996\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uc_vUFJIuDLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}